{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, roc_curve, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 168: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-efa55dfbb6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# data = data.reset_index()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'stopwords-sl.txt'\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mstopwordsSLO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'\\n'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 168: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/'\n",
    "FILENAME = 'AllDiscussionDataCODED_USE_THIS_14Feb2020_MH.xls'\n",
    "\n",
    "data = pd.read_excel( io = DATA_DIR + FILENAME ).dropna('index', 'all').dropna('columns', 'all')\n",
    "# data = data.reset_index()\n",
    "with open( DATA_DIR + 'stopwords-sl.txt', mode='r', encoding='unicode') as f:\n",
    "    stopwordsSLO = f.read().split( '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer( object ):\n",
    "    def __init__( self ):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__( self, articles ):\n",
    "        return [ self.wnl.lemmatize( t ) for t in word_tokenize( articles ) if self.wnl.lemmatize( t ) not in stopwordsSLO ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop( data[ pd.isna( data[ 'Message' ] ) == True ].index, inplace = True ) # remove rows with empty message\n",
    "data.drop( data[ pd.isna( data[ 'Book relevance' ] ) == True ].index, inplace = True ) # remove rows with unknown book relevance\n",
    "\n",
    "# some additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( len( data[ pd.isna( data[ 'Message' ] ) == True ] ) )\n",
    "# print( len( data[ pd.isna( data[ 'Book relevance' ] ) == True ] ) )\n",
    "X_train, X_test, y_train, y_test = train_test_split( data[ 'Message' ], data[ 'Book relevance' ], random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_relevant = len( X_train[ ( data[ 'Book relevance' ] == 'Yes' ) ] )\n",
    "x_nonrelevant = len( X_train[ ( data[ 'Book relevance' ] == 'No' ) ] )\n",
    "testNonRelevant = len( X_test[ ( data[ 'Book relevance' ] == 'No' ) ] )\n",
    "print( 'Book relevant texts in training set: {0}'.format( x_relevant ) )\n",
    "print( 'Book non-relevant texts in training set: {0}'.format( x_nonrelevant ) )\n",
    "print( 'Baseline classifier accuracy on training set: {0}'.format( x_nonrelevant / len( X_train ) ) )\n",
    "print( 'Baseline classifier accuracy on test set: {0}'.format( testNonRelevant / len( X_test ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer( tokenizer = LemmaTokenizer(), analyzer = 'word', ngram_range = ( 1, 3 ), stop_words = stopwordsSLO )\n",
    "cv = CountVectorizer( analyzer = 'word', ngram_range = ( 1, 3 ), stop_words = stopwordsSLO )\n",
    "\n",
    "X_train = np.array( [ value for value in X_train ], dtype = str )\n",
    "y_train = np.array( [ value for value in y_train ], dtype = str )\n",
    "\n",
    "X_test = np.array( [ value for value in X_test ], dtype = str )\n",
    "y_test = np.array( [ value for value in y_test ], dtype = str )\n",
    "\n",
    "# X_train_cv = cv.fit_transform( X_train.values.astype( 'U' ) )\n",
    "# X_test_cv = cv.transform( X_test.values.astype( str ) )\n",
    "\n",
    "X_train_cv = cv.fit_transform( X_train )\n",
    "X_test_cv = cv.transform( X_test )\n",
    "\n",
    "# word_freq_df = pd.DataFrame( X_train_cv.toarray(), columns = cv.get_feature_names() )\n",
    "# top_words_df = pd.DataFrame( word_freq_df.sum() ).sort_values( 0, ascending = False )\n",
    "# print( top_words_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveLabel = 'Yes'\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit( X_train_cv, y_train )\n",
    "predictions = naive_bayes.predict( X_test_cv )\n",
    "\n",
    "print( 'Accuracy score of Naive Bayes: ', accuracy_score( y_test, predictions ) )\n",
    "print( 'Precision score of Naive Bayes: ', precision_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'Recall score of Naive Bayes: ', recall_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print()\n",
    "# cm = confusion_matrix( y_test, predictions )\n",
    "# print( cm )\n",
    "# df_cm = pd.DataFrame( cm, range( 2 ), range( 2 ) )\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sn.set(font_scale=1.4) # for label size\n",
    "# sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "# plt.imshow( cm )\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# probs = naive_bayes.predict_proba( X_test_cv )\n",
    "probs = naive_bayes.predict_proba( X_test_cv )\n",
    "preds = probs[ :, 1 ]\n",
    "fpr, tpr, threshold = roc_curve( y_test, preds, pos_label = positiveLabel )\n",
    "roc_auc = auc( fpr, tpr )\n",
    "\n",
    "plt.title( 'Receiver Operating Characteristic' )\n",
    "plt.plot( fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc )\n",
    "plt.legend( loc = 'lower right') \n",
    "plt.plot( [ 0, 1 ], [ 0, 1 ], 'r--' )\n",
    "plt.xlim( [ 0, 1 ] )\n",
    "plt.ylim( [ 0, 1 ] )\n",
    "plt.ylabel( 'True Positive Rate' )\n",
    "plt.xlabel( 'False Positive Rate' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit( X_train_cv, y_train )\n",
    "predictions = lr.predict( X_test_cv )\n",
    "\n",
    "print( 'Accuracy score of Logistic Regression: ', accuracy_score( y_test, predictions ) )\n",
    "print( 'Precision score of Logistic Regression: ', precision_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'Recall score of Logistic Regression: ', recall_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "\n",
    "probs = lr.predict_proba( X_test_cv )\n",
    "preds = probs[ :, 1 ]\n",
    "fpr, tpr, threshold = roc_curve( y_test, preds, pos_label = positiveLabel )\n",
    "roc_auc = auc( fpr, tpr )\n",
    "\n",
    "plt.title( 'Receiver Operating Characteristic' )\n",
    "plt.plot( fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc )\n",
    "plt.legend( loc = 'lower right') \n",
    "plt.plot( [ 0, 1 ], [ 0, 1 ], 'r--' )\n",
    "plt.xlim( [ 0, 1 ] )\n",
    "plt.ylim( [ 0, 1 ] )\n",
    "plt.ylabel( 'True Positive Rate' )\n",
    "plt.xlabel( 'False Positive Rate' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC( probability = True )\n",
    "svm.fit( X_train_cv, y_train )\n",
    "predictions = svm.predict( X_test_cv )\n",
    "\n",
    "print( 'Accuracy score of Logistic Regression: ', accuracy_score( y_test, predictions ) )\n",
    "print( 'Precision score of Logistic Regression: ', precision_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'Recall score of Logistic Regression: ', recall_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "\n",
    "probs = svm.predict_proba( X_test_cv )\n",
    "preds = probs[ :, 1 ]\n",
    "fpr, tpr, threshold = roc_curve( y_test, preds, pos_label = positiveLabel )\n",
    "roc_auc = auc( fpr, tpr )\n",
    "\n",
    "plt.title( 'Receiver Operating Characteristic' )\n",
    "plt.plot( fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc )\n",
    "plt.legend( loc = 'lower right') \n",
    "plt.plot( [ 0, 1 ], [ 0, 1 ], 'r--' )\n",
    "plt.xlim( [ 0, 1 ] )\n",
    "plt.ylim( [ 0, 1 ] )\n",
    "plt.ylabel( 'True Positive Rate' )\n",
    "plt.xlabel( 'False Positive Rate' )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
