{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, roc_curve, confusion_matrix, plot_confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "TOOLS_DIR = '../tools/'\n",
    "FILENAME = 'AllDiscussionDataCODED_USE_THIS_14Feb2020_MH.xls'\n",
    "LEMMASFILETRAIN = DATA_DIR + 'train.output'\n",
    "LEMMASFILETEST = DATA_DIR + 'test.output'\n",
    "\n",
    "data = pd.read_excel( io = DATA_DIR + FILENAME ).dropna('index', 'all').dropna('columns', 'all')\n",
    "# data = data.reset_index()\n",
    "with open( DATA_DIR + 'stopwords-sl.txt', mode='r', encoding='utf-8') as f:\n",
    "    stopwordsSLO = f.read().split( '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLemmas( filename ):\n",
    "    lemmas = []\n",
    "    with open( filename, mode = 'r', encoding ='utf-8' ) as f:\n",
    "        for l in f.readlines():\n",
    "            line = l.strip()\n",
    "            if line.startswith( '#' ):\n",
    "                if line.startswith( '# newpar id =' ):\n",
    "                    lemmas.append( [ ] )\n",
    "                else:\n",
    "                    continue\n",
    "            tmp = line.split( '\\t' )\n",
    "            if len( tmp ) > 1:\n",
    "                lemmas[ -1 ].append( tmp[ 2 ] )\n",
    "#     return pd.Series( lemmas, indexes )\n",
    "    return lemmas\n",
    "\n",
    "def writeMessages( msgs, filename ):\n",
    "    with open( DATA_DIR + filename + '.input', 'w', encoding='utf-8' ) as f:\n",
    "        for line in msgs:\n",
    "            f.write( str( line ) + '\\n' )\n",
    "\n",
    "def prepareTokens( filename ):\n",
    "    command = 'cat {0}{3}.input | python {1}tokeniser.py sl -n -c -d > {2}'.format( DATA_DIR, TOOLS_DIR + 'reldi-tokeniser-master/', DATA_DIR + filename + '.conllu', filename )\n",
    "    print( 'Running command ', command )\n",
    "    !$command\n",
    "    \n",
    "def prepareLemmas( filename ):\n",
    "    command = 'cd {0} python -m stanfordnlp.models.lemmatizer --model_dir models/lemma/ --model_file ssj500k+Sloleks --eval_file {1} --output_file {2} --mode predict; cd --;'.format( '../tools/classla-stanfordnlp-master;', '../../data/' + filename + '.conllu', '../../data/' + filename + '.output' )\n",
    "    !$command\n",
    "    \n",
    "def lemmatize( messages, filename ): # filename without endings\n",
    "#     filename = 'train' if train else 'test'\n",
    "    \n",
    "    writeMessages( messages, filename )\n",
    "    prepareTokens( filename )\n",
    "    prepareLemmas( filename )\n",
    "    lemmas = parseLemmas( DATA_DIR + filename + '.output' )\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer( object ):\n",
    "    def __init__( self ):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__( self, articles ):\n",
    "#         print( articles )\n",
    "        return articles\n",
    "#         return [ self.wnl.lemmatize( t ) for t in word_tokenize( articles ) if self.wnl.lemmatize( t ) not in stopwordsSLO ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop( data[ pd.isna( data[ 'Message' ] ) == True ].index, inplace = True ) # remove rows with empty message\n",
    "data.drop( data[ pd.isna( data[ 'Book relevance' ] ) == True ].index, inplace = True ) # remove rows with unknown book relevance\n",
    "\n",
    "# some additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "butalci = []\n",
    "with open( DATA_DIR + 'butalski.txt', mode = 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f.readlines():\n",
    "        butalci.append( line.strip() )\n",
    "        \n",
    "vevericek = []\n",
    "with open( DATA_DIR + 'vevericek.txt', mode = 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f.readlines():\n",
    "        vevericek.append( line.strip() )\n",
    "        \n",
    "premrazeno = []\n",
    "with open( DATA_DIR + 'premrazeno.txt', mode = 'r', encoding = 'utf-8' ) as f:\n",
    "    for line in f.readlines():\n",
    "        premrazeno.append( line.strip() )\n",
    "\n",
    "lemmasButalci = parseLemmas( DATA_DIR + 'butalci.output' ) # lemmatize( butalci, 'butalci' )\n",
    "print( len( lemmasButalci ) )\n",
    "\n",
    "lemmasVevericek = parseLemmas( DATA_DIR + 'vevericek.output' ) # lemmatize( vevericek, 'vevericek' )\n",
    "print( len( lemmasVevericek ) )\n",
    "\n",
    "lemmasPremrazeno = parseLemmas( DATA_DIR + 'premrazeno.output' ) # lemmatize( premrazeno, 'premrazeno' )\n",
    "print( len( lemmasPremrazeno ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( len( data[ pd.isna( data[ 'Message' ] ) == True ] ) )\n",
    "# print( len( data[ pd.isna( data[ 'Book relevance' ] ) == True ] ) )\n",
    "X_trainO, X_testO, y_trainO, y_testO = train_test_split( data[ 'Message' ], data[ 'Book relevance' ], random_state = 42 )\n",
    "# writeMessages( X_trainO )\n",
    "# prepareTokens()\n",
    "# prepareLemmas()\n",
    "\n",
    "lemmasTrain = parseLemmas( LEMMASFILETRAIN )\n",
    "lemmasTest = parseLemmas( LEMMASFILETEST )\n",
    "# lemmas = lemmatize( X_trainO, 'train' )\n",
    "# lemmas = lemmatize( X_testO, 'test' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_relevant = len( X_trainO[ ( data[ 'Book relevance' ] == 'Yes' ) ] )\n",
    "x_nonrelevant = len( X_trainO[ ( data[ 'Book relevance' ] == 'No' ) ] )\n",
    "testNonRelevant = len( X_testO[ ( data[ 'Book relevance' ] == 'No' ) ] )\n",
    "print( 'Book relevant texts in training set: {0}'.format( x_relevant ) )\n",
    "print( 'Book non-relevant texts in training set: {0}'.format( x_nonrelevant ) )\n",
    "print( 'Baseline classifier accuracy on training set: {0}'.format( x_nonrelevant / len( X_trainO ) ) )\n",
    "print( 'Baseline classifier accuracy on test set: {0}'.format( testNonRelevant / len( X_testO ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MeasureFeatures(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['length', 'contains_question_mark', 'contains_exclamation_mark'])\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        \n",
    "        X_length = list()\n",
    "        X_contains_question_mark = list()\n",
    "        X_contains_exclamation_mark = list()\n",
    "        \n",
    "        for sentence in x_dataset:\n",
    "            \n",
    "            X_length.append(len(sentence))\n",
    "            X_contains_question_mark.append(1 if str(sentence).find(\"?\") != -1 else 0)\n",
    "            X_contains_exclamation_mark.append(1 if str(sentence).find(\"!\") != -1 else 0)\n",
    "\n",
    "            # takes pos tag text and counts number of noun pos tags (NN, NNS etc.)\n",
    "\n",
    "        X = np.array([X_length, X_contains_question_mark, X_contains_exclamation_mark]).T\n",
    "\n",
    "        if not hasattr(self, 'scalar'):\n",
    "            self.scalar = StandardScaler().fit(X)\n",
    "        return self.scalar.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer( tokenizer = LemmaTokenizer(), analyzer = 'word', ngram_range = ( 1, 3 ), stop_words = stopwordsSLO )\n",
    "# cv = CountVectorizer( analyzer = 'word', ngram_range = ( 1, 3 ), stop_words = stopwordsSLO )\n",
    "\n",
    "# cv = CountVectorizer(  tokenizer = lambda x : x, preprocessor = lambda x : x, stop_words = stopwordsSLO )\n",
    "\n",
    "'''cv = FeatureUnion([('feat', MeasureFeatures()),\n",
    "                   ('tfidf', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words=stopwordsSLO))\n",
    "                  ])'''\n",
    "cv = CountVectorizer( tokenizer = lambda x : x, preprocessor = lambda x : x )\n",
    "\n",
    "X_train = np.array( [ value for value in X_trainO ], dtype = str )\n",
    "y_train = np.array( [ value for value in y_trainO ], dtype = str )\n",
    "\n",
    "X_test = np.array( [ value for value in X_testO ], dtype = str )\n",
    "y_test = np.array( [ value for value in y_testO ], dtype = str )\n",
    "\n",
    "# X_train_cv = cv.fit_transform( X_train.values.astype( 'U' ) )\n",
    "# X_test_cv = cv.transform( X_test.values.astype( str ) )\n",
    "\n",
    "# X_train_cv = cv.fit_transform( X_train )\n",
    "# X_test_cv = cv.transform( X_test )\n",
    "\n",
    "X_train_cv = cv.fit_transform( lemmasTrain )\n",
    "X_test_cv = cv.transform( lemmasTest )\n",
    "\n",
    "# word_freq_df = pd.DataFrame( X_train_cv.toarray(), columns = cv.get_feature_names() )\n",
    "# top_words_df = pd.DataFrame( word_freq_df.sum() ).sort_values( 0, ascending = False )\n",
    "# print( top_words_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveLabel = 'Yes'\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit( X_train_cv, y_train )\n",
    "predictions = naive_bayes.predict( X_test_cv )\n",
    "\n",
    "print( 'Accuracy score of Naive Bayes: ', accuracy_score( y_test, predictions ) )\n",
    "print( 'Precision score of Naive Bayes: ', precision_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'Recall score of Naive Bayes: ', recall_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'F1 score of Naive Bayes:', f1_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print()\n",
    "\n",
    "\n",
    "# cm = confusion_matrix( y_test, predictions )\n",
    "# print( cm )\n",
    "# df_cm = pd.DataFrame( cm, range( 2 ), range( 2 ) )\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sn.set(font_scale=1.4) # for label size\n",
    "# sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "# plt.imshow( cm )\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# probs = naive_bayes.predict_proba( X_test_cv )\n",
    "probs = naive_bayes.predict_proba( X_test_cv )\n",
    "preds = probs[ :, 1 ]\n",
    "fpr, tpr, threshold = roc_curve( y_test, preds, pos_label = positiveLabel )\n",
    "roc_auc = auc( fpr, tpr )\n",
    "\n",
    "plt.title( 'Receiver Operating Characteristic' )\n",
    "plt.plot( fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc )\n",
    "plt.legend( loc = 'lower right') \n",
    "plt.plot( [ 0, 1 ], [ 0, 1 ], 'r--' )\n",
    "plt.xlim( [ 0, 1 ] )\n",
    "plt.ylim( [ 0, 1 ] )\n",
    "plt.ylabel( 'True Positive Rate' )\n",
    "plt.xlabel( 'False Positive Rate' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrRelevant = LogisticRegression()\n",
    "lrRelevant.fit( X_train_cv, y_train )\n",
    "predictions = lrRelevant.predict( X_test_cv )\n",
    "\n",
    "print( 'Accuracy score of Logistic Regression: ', accuracy_score( y_test, predictions ) )\n",
    "print( 'Precision score of Logistic Regression: ', precision_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'Recall score of Logistic Regression: ', recall_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'F1 score of Logistic Regression:', f1_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "\n",
    "probs = lrRelevant.predict_proba( X_test_cv )\n",
    "preds = probs[ :, 1 ]\n",
    "fpr, tpr, threshold = roc_curve( y_test, preds, pos_label = positiveLabel )\n",
    "roc_auc = auc( fpr, tpr )\n",
    "\n",
    "plt.title( 'Receiver Operating Characteristic' )\n",
    "plt.plot( fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc )\n",
    "plt.legend( loc = 'lower right') \n",
    "plt.plot( [ 0, 1 ], [ 0, 1 ], 'r--' )\n",
    "plt.xlim( [ 0, 1 ] )\n",
    "plt.ylim( [ 0, 1 ] )\n",
    "plt.ylabel( 'True Positive Rate' )\n",
    "plt.xlabel( 'False Positive Rate' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC( probability = True )\n",
    "svm.fit( X_train_cv, y_train )\n",
    "predictions = svm.predict( X_test_cv )\n",
    "\n",
    "print( 'Accuracy score of SVM: ', accuracy_score( y_test, predictions ) )\n",
    "print( 'Precision score of SVM: ', precision_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'Recall score of SVM: ', recall_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "print( 'F1 score of SVM:', f1_score( y_test, predictions, pos_label = positiveLabel ) )\n",
    "\n",
    "probs = svm.predict_proba( X_test_cv )\n",
    "preds = probs[ :, 1 ]\n",
    "fpr, tpr, threshold = roc_curve( y_test, preds, pos_label = positiveLabel )\n",
    "roc_auc = auc( fpr, tpr )\n",
    "\n",
    "plt.title( 'Receiver Operating Characteristic' )\n",
    "plt.plot( fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc )\n",
    "plt.legend( loc = 'lower right') \n",
    "plt.plot( [ 0, 1 ], [ 0, 1 ], 'r--' )\n",
    "plt.xlim( [ 0, 1 ] )\n",
    "plt.ylim( [ 0, 1 ] )\n",
    "plt.ylabel( 'True Positive Rate' )\n",
    "plt.xlabel( 'False Positive Rate' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [ data[ 'CategoryBroad'][ i ] for i in X_trainO.index ]\n",
    "y_test = [ data[ 'CategoryBroad'][ i ] for i in X_testO.index ]\n",
    "\n",
    "labels = set( [ value for value in data[ 'CategoryBroad'] ] )\n",
    "for label in labels:\n",
    "    print( 'Number of examples in category {0}: {1}'.format( label, len( X_trainO[ ( data[ 'CategoryBroad' ] == label ) ] ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit( X_train_cv, y_train )\n",
    "predictions = naive_bayes.predict( X_test_cv )\n",
    "print( 'Accuracy score of Naive Bayes: ', accuracy_score( y_test, predictions ) )\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit( X_train_cv, y_train )\n",
    "predictions = lr.predict( X_test_cv )\n",
    "print( 'Accuracy score of Logistic Regression: ', accuracy_score( y_test, predictions ) )\n",
    "\n",
    "svm = SVC( probability = True )\n",
    "svm.fit( X_train_cv, y_train )\n",
    "predictions = svm.predict( X_test_cv )\n",
    "print( 'Accuracy score of SVM: ', accuracy_score( y_test, predictions ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bla = lrRelevant.predict( X_test_cv )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
